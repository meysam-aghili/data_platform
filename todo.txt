-------KAFKA QA
aya port dakheli expose shode yek image baraye yek containeri ke ba compose file dg vali network yeksan oomade bala baze?
	age are pas chra port metric exporter connect ro map kardi be biron?
chra baazi jaha ye port ro 2bar avordi biron?
artin kafka volumes chra yekie baraye controller ha va broker ha
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT mishe CONTROLLER:PLAINTEXT ro hazv kard?
chra port internal va external kafka ro keshidi biron
chra port controller kafka ro keshidi biron
kafka-cli be dard mikhore
kafka-rest be dard mikhore
kafdrop ya kafka ui kodomesh?
tedad partition haro chejuri mohasebe mikonid , orders chanta partition dare?
age ye broker fail she replication factor dobare emal mishe? partition ha chi mishn?
reconsilation chie
get sample connector artin
strategy consume haye mokhtalef mesle range ya sticky karbord dare?
acks ro gozashti -1 yani chi age alle ykm tool nmikeshe?
idempotence chie?
az kafka transaction estefade kardi?
data imbalance ro chejuri handle mikonid
image ksqldb ksqldb-server ksql-server cp-ksqldb which one ?
deadletterqueue darid?

-------CLICKHOUSE QA
che datai too click hosue ngeah midarid ? khame orders ya agg data of orders?
ttl table ha mamoln chejuri mizarid? maslan table orders
az aggmergetree estefade mikonid? ya hamon mergetree mizanid va roosh query agg mizanid
chanta broker node va keeper node darid? chanta sahrd chanta replica darid?
az kafka engine ya sink connector?

--------------------------------------
visual data using grafana, metabase, powerbi, tableau, supercet

add logs to elk stack and visual
add influx for time series data and visual
add minio
add spark
test dagster
ksql
sasl and security


---------------------------

"keeperOnCluster": ""

CREATE DATABASE shop;

CREATE TABLE shop.users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at DateTime,
    created_at DateTime,
    kafka_time Nullable(DateTime),
    kafka_offset UInt64
)
ENGINE = ReplacingMergeTree
ORDER BY (user_id, updated_at)
SETTINGS index_granularity = 8192

CREATE DATABASE kafka_shop;

CREATE TABLE kafka_shop.kafka__users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at UInt64,
    created_at UInt64
)
ENGINE = Kafka
SETTINGS kafka_broker_list = 'broker:29092',
kafka_topic_list = 'shop.public.users',
kafka_group_name = 'clickhouse',
kafka_format = 'AvroConfluent',
format_avro_schema_registry_url='http://schema-registry:8081';

CREATE MATERIALIZED VIEW kafka_shop.consumer__users TO shop.users
(
    user_id UInt32,
    username String,
    account_type String,
    updated_at DateTime,
    created_at DateTime,
    kafka_time Nullable(DateTime),
    kafka_offset UInt64
) AS
SELECT
    user_id,
    username,
    account_type,
    toDateTime(updated_at / 1000000) AS updated_at,
    toDateTime(created_at / 1000000) AS created_at,
    _timestamp AS kafka_time,
    _offset AS kafka_offset
FROM kafka_shop.kafka__users;


{
  "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
  "tasks.max": "1",
  "topics": "orders",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "value.converter.schema.registry.url": "http://schema-registry:8081",
  "value.converter.enhanced.avro.schema.support": true,
  "key.converter": "org.apache.kafka.connect.storage.StringConverter",
  "connection.url": "jdbc:mysql://mysql:3306/test_db",
  "connection.user": "mysql",
  "connection.password": "mysql",
  "transforms": "unwrap",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.unwrap.drop.tombstones": "false",

  "table.name.format": "orders",
  "pk.mode": "record_value",
  "pk.fields": "id",
  "insert.mode": "upsert",
 
  "auto.create": "true",
  "errors.tolerance": "all",
  "errors.log.enable": "true",
  "errors.log.include.messages": "true"
}

