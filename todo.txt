-------KAFKA QA
aya port dakheli expose shode yek image baraye yek containeri ke ba compose file dg vali network yeksan oomade bala baze?
	age are pas chra port metric exporter connect ro map kardi be biron?
chra baazi jaha ye port ro 2bar avordi biron?
artin kafka volumes chra yekie baraye controller ha va broker ha
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT mishe CONTROLLER:PLAINTEXT ro hazv kard?
chra port internal va external kafka ro keshidi biron
chra port controller kafka ro keshidi biron
kafka-cli be dard mikhore
kafka-rest be dard mikhore
kafdrop ya kafka ui kodomesh?
tedad partition haro chejuri mohasebe mikonid , orders chanta partition dare?
age ye broker fail she replication factor dobare emal mishe? partition ha chi mishn?
reconsilation chie
get sample connector artin
strategy consume haye mokhtalef mesle range ya sticky karbord dare?
acks ro gozashti -1 yani chi age alle ykm tool nmikeshe?
idempotence chie?
az kafka transaction estefade kardi?
data imbalance ro chejuri handle mikonid

-------CLICKHOUSE QA
che datai too click hosue ngeah midarid ? khame orders ya agg data of orders?
ttl table ha mamoln chejuri mizarid? maslan table orders
optimize table final ?
index_granularity option ? index types?
primary key vs order by deatil?
explain indexes=1 ?
Adding a projection to our existing table. what is that?
ALL DDL AND QUERY TIPS
data skipping index?
projection?
az aggmergetree estefade mikonid? ya hamon mergetree mizanid va roosh query agg mizanid


setup clickhouse
 	https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication
	https://clickhouse.com/docs/en/architecture/introduction
	https://github.com/ClickHouse/examples/blob/main/docker-compose-recipes/README.md
	https://clickhouse.com/docs/en/use-cases/observability/introduction

add postgres data to clickhouse
visual data using grafana, metabase, powerbi, tableau, supercet

add logs to elk stack and visual
add influx for time series data and visual
add minio
add spark

test dagster

ksql
sasl and security





SELECT
    part_type,
    path,
    formatReadableQuantity(rows) AS rows,
    formatReadableSize(data_uncompressed_bytes) AS data_uncompressed_bytes,
    formatReadableSize(data_compressed_bytes) AS data_compressed_bytes,
    formatReadableSize(primary_key_bytes_in_memory) AS primary_key_bytes_in_memory,
    marks,
    formatReadableSize(bytes_on_disk) AS bytes_on_disk
FROM system.parts
WHERE (table = 'hits_UserID_URL') AND (active = 1)
FORMAT Vertical;

{
  "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
  "tasks.max": "1",
  "topics": "orders",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "value.converter.schema.registry.url": "http://schema-registry:8081",
  "value.converter.enhanced.avro.schema.support": true,
  "key.converter": "org.apache.kafka.connect.storage.StringConverter",
  "connection.url": "jdbc:mysql://mysql:3306/test_db",
  "connection.user": "mysql",
  "connection.password": "mysql",
  "transforms": "unwrap",
  "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
  "transforms.unwrap.drop.tombstones": "false",

  "table.name.format": "orders",
  "pk.mode": "record_value",
  "pk.fields": "id",
  "insert.mode": "upsert",
 
  "auto.create": "true",
  "errors.tolerance": "all",
  "errors.log.enable": "true",
  "errors.log.include.messages": "true"
}

